{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from hessian import hessian\n",
    "from hessian_eigenthings.power_iter import Operator, deflated_power_iteration\n",
    "from hessian_eigenthings.lanczos import lanczos\n",
    "from lanczos_generalized import lanczos_generalized\n",
    "from GAN_hvp_operator import GANHVPOperator, compute_hessian_eigenthings\n",
    "import numpy as np\n",
    "from time import time\n",
    "from imageio import imwrite\n",
    "from build_montages import build_montages\n",
    "import matplotlib.pylab as plt\n",
    "from os.path import join"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from GAN_utils import upconvGAN\n",
    "G = upconvGAN(\"fc6\")\n",
    "G.requires_grad_(False).cuda() # this notation is incorrect in older pytorch\n",
    "#%\n",
    "import torchvision as tv\n",
    "# VGG = tv.models.vgg16(pretrained=True)\n",
    "alexnet = tv.models.alexnet(pretrained=True).cuda()\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad_(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code shows this is not working.... The local 2nd order derivative is 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "feat = torch.randn(4096).cuda()\n",
    "feat.requires_grad_(True)\n",
    "weight = torch.randn(192, 31, 31).cuda()\n",
    "objective = FeatLinModel(alexnet, layername='features_4', type=\"weight\", weight=weight)\n",
    "act = objective(G.visualize(feat))\n",
    "gradient = torch.autograd.grad(act, feat, retain_graph=True, create_graph=True,)\n",
    "torch.autograd.grad(gradient[0], feat, retain_graph=True, only_inputs=True, grad_outputs=10*torch.ones(4096).cuda())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat = torch.tensor(np.random.randn(4096)).float().cuda()\n",
    "feat.requires_grad_(True)\n",
    "img = G.visualize(feat)\n",
    "fc8 = alexnet.forward(img)\n",
    "act = - fc8[0, 1]\n",
    "H = hessian(act, feat, create_graph=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat = torch.tensor(np.random.randn(4096)).float().cuda()\n",
    "feat.requires_grad_(True)\n",
    "img = G.visualize(feat)\n",
    "act = - img.mean()\n",
    "# fc8 = alexnet.forward(img)\n",
    "# act = - fc8[0, 1]\n",
    "# H = hessian(act, feat, create_graph=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gradient = torch.autograd.grad(act, feat, retain_graph=True, create_graph=True,)\n",
    "torch.autograd.grad(gradient[0], feat, retain_graph=True, only_inputs=True, grad_outputs=10*torch.ones(4096).cuda())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "H = hessian(act, feat, create_graph=False)\n",
    "# it will be all zero\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Compute the full hessian using hessian package\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0,2])\n",
    "x.requires_grad_(True)\n",
    "A = torch.tensor([[2.0, 3], [3, 1]])\n",
    "y = x.view(1, -1)@A@x.view(-1, 1)\n",
    "x_grad = torch.autograd.grad(y, x, retain_graph=True, create_graph=True)\n",
    "torch.autograd.grad(x_grad, x, retain_graph=True, only_inputs=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat = torch.tensor(np.random.randn(4096)).float().cuda()\n",
    "feat.requires_grad_(True)\n",
    "img = G.visualize(feat)\n",
    "resz_img = F.interpolate(img, (224, 224), mode='bilinear', align_corners=True)\n",
    "obj = alexnet.features[:10](resz_img)[0, :, 6, 6].mean().pow(2)  # esz_img.std()\n",
    "ftgrad = torch.autograd.grad(obj, feat, retain_graph=True, create_graph=True, only_inputs=True)\n",
    "torch.autograd.grad(1 * ftgrad[0], feat, retain_graph=True, only_inputs=True, grad_outputs=torch.randn(4096).cuda(), )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So here is the conclusion:\n",
    "* If the objective function is linear to the neural activation in a ReLU NN, then the 2nd order derivative will be 0 analytically. We need finite differencing for this case.\n",
    "* As the Perceptual loss take a squared difference when comparing\n",
    "feature tensros, the dependency of loss on image is power 2, thus the derivative of it is not independent of image."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is literature that proves the functional family that ReLU NN can represent IS exactly piecewise linear functions. So it's not surprising that the 2nd derivative is 0 almost everywhere. And using finite differencing is in effect computing the Hessian of a smoothed version of this function.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/6/6d/Piecewise_linear_function2D.svg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So next, I show the feasibility of using forward differencing to compute the approximate **Hessian vector product**. The formula is simply based on Taylor expansion\n",
    "\n",
    "$$Hv \\approx {g(x+\\epsilon v) - g(x-\\epsilon v)) \\over 2\\epsilon}$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat = torch.tensor(np.random.randn(4096)).float().cuda()\n",
    "feat.requires_grad_(False)\n",
    "vect = torch.tensor(np.random.randn(4096)).float().cuda()\n",
    "vect = vect / vect.norm()\n",
    "vect.requires_grad_(False)\n",
    "hvp_col = []\n",
    "for eps in [100, 50, 25, 10, 5, 1, 5E-1, 1E-1, 1E-2, 1E-3, 1E-4, 1E-5, 1E-6, ]:\n",
    "    perturb_vecs = feat.detach() + eps * torch.tensor([1, -1.0]).view(-1, 1).cuda() * vect.detach()\n",
    "    perturb_vecs.requires_grad_(True)\n",
    "    img = G.visualize(perturb_vecs)\n",
    "    resz_img = F.interpolate(img, (224, 224), mode='bilinear', align_corners=True)\n",
    "    obj = alexnet.features[:10](resz_img)[:, :, 6, 6].mean()   # esz_img.std()\n",
    "    ftgrad_both = torch.autograd.grad(obj, perturb_vecs, retain_graph=False, create_graph=False, only_inputs=True)\n",
    "    hvp = (ftgrad_both[0][0, :] - ftgrad_both[0][1, :]) / (2 * eps)\n",
    "    hvp_col.append(hvp)\n",
    "    # print(hvp)\n",
    "\n",
    "hvp_arr = torch.cat(tuple(hvp.unsqueeze(0) for hvp in hvp_col), dim=0)\n",
    "corrmat = np.corrcoef(hvp_arr.cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.matshow(corrmat, cmap=plt.cm.jet)\n",
    "plt.yticks(range(12), labels=[50, 25, 10, 5, 1, 5E-1, 1E-1, 1E-2, 1E-3, 1E-4, 1E-5, 1E-6, ])\n",
    "plt.xticks(range(12), labels=[50, 25, 10, 5, 1, 5E-1, 1E-1, 1E-2, 1E-3, 1E-4, 1E-5, 1E-6, ])\n",
    "plt.ylim(top = -0.5, bottom=11.5)\n",
    "plt.xlim(left = -0.5, right=11.5)\n",
    "plt.xlabel(\"Perturb Vector Norm (Base Vector Norm 300)\")\n",
    "plt.suptitle(\"Correlation of HVP result (500 Trials)\\nusing different EPS in forward differencing\\n\")\n",
    "plt.colorbar()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-.conda-caffe36-py",
   "language": "python",
   "display_name": "Python [conda env:.conda-caffe36] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}